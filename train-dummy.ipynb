{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train-dummy.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"qLKEc-Vn1Wmq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"3370d70b-3b3f-4221-e288-58c3493f116b","executionInfo":{"status":"error","timestamp":1539531182088,"user_tz":-330,"elapsed":3124,"user":{"displayName":"Manoj K Sharma","photoUrl":"","userId":"14163422646141132762"}}},"cell_type":"code","source":["import tensorflow as tf\n","import tensorlayer as tl\n","from tensorlayer.layers import *\n","from tensorlayer.prepro import *\n","from tensorlayer.cost import *\n","import numpy as np\n","import scipy\n","from scipy.io import loadmat\n","import time, os, re, nltk\n","\n","from utils import *\n","from model import *\n","import model\n","\n","\n","###======================== PREPARE DATA ====================================###\n","print(\"Loading data from pickle ...\")\n","import pickle\n","with open(\"_vocab.pickle\", 'rb') as f:\n","    vocab = pickle.load(f)\n","with open(\"_image_train.pickle\", 'rb') as f:\n","    _, images_train = pickle.load(f)\n","with open(\"_image_test.pickle\", 'rb') as f:\n","    _, images_test = pickle.load(f)\n","with open(\"_n.pickle\", 'rb') as f:\n","    n_captions_train, n_captions_test, n_captions_per_image, n_images_train, n_images_test = pickle.load(f)\n","with open(\"_caption.pickle\", 'rb') as f:\n","    captions_ids_train, captions_ids_test = pickle.load(f)\n","# images_train_256 = np.array(images_train_256)\n","# images_test_256 = np.array(images_test_256)\n","images_train = np.array(images_train)\n","images_test = np.array(images_test)\n","\n","# print(n_captions_train, n_captions_test)\n","# exit()\n","\n","ni = int(np.ceil(np.sqrt(batch_size)))\n","# os.system(\"mkdir samples\")\n","# os.system(\"mkdir samples/step1_gan-cls\")\n","# os.system(\"mkdir checkpoint\")\n","tl.files.exists_or_mkdir(\"samples/step1_gan-cls\")\n","tl.files.exists_or_mkdir(\"samples/step_pretrain_encoder\")\n","tl.files.exists_or_mkdir(\"checkpoint\")\n","save_dir = \"checkpoint\"\n","\n","\n","def main_train():\n","    ###======================== DEFIINE MODEL ===================================###\n","    t_real_image = tf.placeholder('float32', [batch_size, image_size, image_size, 3], name = 'real_image')\n","    t_wrong_image = tf.placeholder('float32', [batch_size ,image_size, image_size, 3], name = 'wrong_image')\n","    t_real_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='real_caption_input')\n","    t_wrong_caption = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name='wrong_caption_input')\n","    t_z = tf.placeholder(tf.float32, [batch_size, z_dim], name='z_noise')\n","\n","    ## training inference for text-to-image mapping\n","    net_cnn = cnn_encoder(t_real_image, is_train=True, reuse=False)\n","    x = net_cnn.outputs\n","    v = rnn_embed(t_real_caption, is_train=True, reuse=False).outputs\n","    x_w = cnn_encoder(t_wrong_image, is_train=True, reuse=True).outputs\n","    v_w = rnn_embed(t_wrong_caption, is_train=True, reuse=True).outputs\n","\n","    alpha = 0.2 # margin alpha\n","    rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n","                tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))\n","\n","    ## training inference for txt2img\n","    generator_txt2img = model.generator_txt2img_resnet\n","    discriminator_txt2img = model.discriminator_txt2img_resnet\n","\n","    net_rnn = rnn_embed(t_real_caption, is_train=False, reuse=True)\n","    net_fake_image, _ = generator_txt2img(t_z,\n","                    net_rnn.outputs,\n","                    is_train=True, reuse=False, batch_size=batch_size)\n","                    #+ tf.random_normal(shape=net_rnn.outputs.get_shape(), mean=0, stddev=0.02), # NOISE ON RNN\n","    net_d, disc_fake_image_logits = discriminator_txt2img(\n","                    net_fake_image.outputs, net_rnn.outputs, is_train=True, reuse=False)\n","    _, disc_real_image_logits = discriminator_txt2img(\n","                    t_real_image, net_rnn.outputs, is_train=True, reuse=True)\n","    _, disc_mismatch_logits = discriminator_txt2img(\n","                    # t_wrong_image,\n","                    t_real_image,\n","                    # net_rnn.outputs,\n","                    rnn_embed(t_wrong_caption, is_train=False, reuse=True).outputs,\n","                    is_train=True, reuse=True)\n","\n","    ## testing inference for txt2img\n","    net_g, _ = generator_txt2img(t_z,\n","                    rnn_embed(t_real_caption, is_train=False, reuse=True).outputs,\n","                    is_train=False, reuse=True, batch_size=batch_size)\n","\n","    d_loss1 = tl.cost.sigmoid_cross_entropy(disc_real_image_logits, tf.ones_like(disc_real_image_logits), name='d1')\n","    d_loss2 = tl.cost.sigmoid_cross_entropy(disc_mismatch_logits,  tf.zeros_like(disc_mismatch_logits), name='d2')\n","    d_loss3 = tl.cost.sigmoid_cross_entropy(disc_fake_image_logits, tf.zeros_like(disc_fake_image_logits), name='d3')\n","    d_loss = d_loss1 + (d_loss2 + d_loss3) * 0.5\n","    g_loss = tl.cost.sigmoid_cross_entropy(disc_fake_image_logits, tf.ones_like(disc_fake_image_logits), name='g')\n","\n","    ####======================== DEFINE TRAIN OPTS ==============================###\n","    lr = 0.0002\n","    lr_decay = 0.5      # decay factor for adam, https://github.com/reedscot/icml2016/blob/master/main_cls_int.lua  https://github.com/reedscot/icml2016/blob/master/scripts/train_flowers.sh\n","    decay_every = 100   # https://github.com/reedscot/icml2016/blob/master/main_cls.lua\n","    beta1 = 0.5\n","\n","    cnn_vars = tl.layers.get_variables_with_name('cnn', True, True)\n","    rnn_vars = tl.layers.get_variables_with_name('rnn', True, True)\n","    d_vars = tl.layers.get_variables_with_name('discriminator', True, True)\n","    g_vars = tl.layers.get_variables_with_name('generator', True, True)\n","\n","    with tf.variable_scope('learning_rate'):\n","        lr_v = tf.Variable(lr, trainable=False)\n","    d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars )\n","    g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars )\n","    # e_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(e_loss, var_list=e_vars + c_vars)\n","    grads, _ = tf.clip_by_global_norm(tf.gradients(rnn_loss, rnn_vars + cnn_vars), 10)\n","    optimizer = tf.train.AdamOptimizer(lr_v, beta1=beta1)# optimizer = tf.train.GradientDescentOptimizer(lre)\n","    rnn_optim = optimizer.apply_gradients(zip(grads, rnn_vars + cnn_vars))\n","\n","    # adam_vars = tl.layers.get_variables_with_name('Adam', False, True)\n","\n","    ###============================ TRAINING ====================================###\n","    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n","    tl.layers.initialize_global_variables(sess)\n","\n","    # load the latest checkpoints\n","    net_rnn_name = os.path.join(save_dir, 'net_rnn.npz')\n","    net_cnn_name = os.path.join(save_dir, 'net_cnn.npz')\n","    net_g_name = os.path.join(save_dir, 'net_g.npz')\n","    net_d_name = os.path.join(save_dir, 'net_d.npz')\n","\n","    load_and_assign_npz(sess=sess, name=net_rnn_name, model=net_rnn)\n","    load_and_assign_npz(sess=sess, name=net_cnn_name, model=net_cnn)\n","    load_and_assign_npz(sess=sess, name=net_g_name, model=net_g)\n","    load_and_assign_npz(sess=sess, name=net_d_name, model=net_d)\n","\n","    ## seed for generation, z and sentence ids\n","    sample_size = batch_size\n","    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n","        # sample_seed = np.random.uniform(low=-1, high=1, size=(sample_size, z_dim)).astype(np.float32)]\n","    n = int(sample_size / ni)\n","    sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * n + \\\n","                      [\"this flower has petals that are yellow, white and purple and has dark lines\"] * n + \\\n","                      [\"the petals on this flower are white with a yellow center\"] * n + \\\n","                      [\"this flower has a lot of small round pink petals.\"] * n + \\\n","                      [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * n + \\\n","                      [\"the flower has yellow petals and the center of it is brown.\"] * n + \\\n","                      [\"this flower has petals that are blue and white.\"] * n +\\\n","                      [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * n\n","\n","    # sample_sentence = captions_ids_test[0:sample_size]\n","    for i, sentence in enumerate(sample_sentence):\n","        print(\"seed: %s\" % sentence)\n","        sentence = preprocess_caption(sentence)\n","        sample_sentence[i] = [vocab.word_to_id(word) for word in nltk.tokenize.word_tokenize(sentence)] + [vocab.end_id]    # add END_ID\n","        # sample_sentence[i] = [vocab.word_to_id(word) for word in sentence]\n","        # print(sample_sentence[i])\n","    sample_sentence = tl.prepro.pad_sequences(sample_sentence, padding='post')\n","\n","    n_epoch = 100 # 600\n","    print_freq = 1\n","    n_batch_epoch = int(n_images_train / batch_size)\n","    # exit()\n","    for epoch in range(0, n_epoch+1):\n","        start_time = time.time()\n","\n","        if epoch !=0 and (epoch % decay_every == 0):\n","            new_lr_decay = lr_decay ** (epoch // decay_every)\n","            sess.run(tf.assign(lr_v, lr * new_lr_decay))\n","            log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n","            print(log)\n","            # logging.debug(log)\n","        elif epoch == 0:\n","            log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n","            print(log)\n","\n","        for step in range(n_batch_epoch):\n","            step_time = time.time()\n","            ## get matched text\n","            idexs = get_random_int(min=0, max=518-1, number=batch_size)\n","            b_real_caption = captions_ids_train[idexs]\n","            b_real_caption = tl.prepro.pad_sequences(b_real_caption, padding='post')\n","            ## get real image\n","            b_real_images = images_train[np.floor(np.asarray(idexs).astype('float')/n_captions_per_image).astype('int')]\n","            # save_images(b_real_images, [ni, ni], 'samples/step1_gan-cls/train_00.png')\n","            ## get wrong caption\n","            idexs = get_random_int(min=0, max=n_captions_train-1, number=batch_size)\n","            b_wrong_caption = captions_ids_train[idexs]\n","            b_wrong_caption = tl.prepro.pad_sequences(b_wrong_caption, padding='post')\n","            ## get wrong image\n","            idexs2 = get_random_int(min=0, max=n_images_train-1, number=batch_size)\n","            b_wrong_images = images_train[idexs2]\n","            ## get noise\n","            #b_z = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, z_dim)).astype(np.float32)\n","            b_z = np.random.uniform(low=-1, high=1, size=[batch_size, z_dim]).astype(np.float32)\n","            b_z[b_z>0] = 1\n","\t\t    b_z[b_z<0] = 0\n","            \n","\n","            b_real_images = threading_data(b_real_images, prepro_img, mode='train')   # [0, 255] --> [-1, 1] + augmentation\n","            b_wrong_images = threading_data(b_wrong_images, prepro_img, mode='train')\n","            ## updates text-to-image mapping\n","            if epoch < 50:\n","                errRNN, _ = sess.run([rnn_loss, rnn_optim], feed_dict={\n","                                                t_real_image : b_real_images,\n","                                                t_wrong_image : b_wrong_images,\n","                                                t_real_caption : b_real_caption,\n","                                                t_wrong_caption : b_wrong_caption})\n","            else:\n","                errRNN = 0\n","\n","            ## updates D\n","            errD, _ = sess.run([d_loss, d_optim], feed_dict={\n","                            t_real_image : b_real_images,\n","                            # t_wrong_image : b_wrong_images,\n","                            t_wrong_caption : b_wrong_caption,\n","                            t_real_caption : b_real_caption,\n","                            t_z : b_z})\n","            ## updates G\n","            errG, _ = sess.run([g_loss, g_optim], feed_dict={\n","                            t_real_caption : b_real_caption,\n","                            t_z : b_z})\n","\n","            print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.8f, g_loss: %.8f, rnn_loss: %.8f\" \\\n","                        % (epoch, n_epoch, step, n_batch_epoch, time.time() - step_time, errD, errG, errRNN))\n","\n","        if (epoch + 1) % print_freq == 0:\n","            print(\" ** Epoch %d took %fs\" % (epoch, time.time()-start_time))\n","            img_gen, rnn_out = sess.run([net_g.outputs, net_rnn.outputs], feed_dict={\n","                                        t_real_caption : sample_sentence,\n","                                        t_z : sample_seed})\n","\n","            # img_gen = threading_data(img_gen, prepro_img, mode='rescale')\n","            save_images(img_gen, [ni, ni], 'samples/step1_gan-cls/train_{:02d}.png'.format(epoch))\n","\n","        ## save model\n","        if (epoch != 0) and (epoch % 10) == 0:\n","            tl.files.save_npz(net_cnn.all_params, name=net_cnn_name, sess=sess)\n","            tl.files.save_npz(net_rnn.all_params, name=net_rnn_name, sess=sess)\n","            tl.files.save_npz(net_g.all_params, name=net_g_name, sess=sess)\n","            tl.files.save_npz(net_d.all_params, name=net_d_name, sess=sess)\n","            print(\"[*] Save checkpoints SUCCESS!\")\n","\n","        if (epoch != 0) and (epoch % 100) == 0:\n","            tl.files.save_npz(net_cnn.all_params, name=net_cnn_name+str(epoch), sess=sess)\n","            tl.files.save_npz(net_rnn.all_params, name=net_rnn_name+str(epoch), sess=sess)\n","            tl.files.save_npz(net_g.all_params, name=net_g_name+str(epoch), sess=sess)\n","            tl.files.save_npz(net_d.all_params, name=net_d_name+str(epoch), sess=sess)\n","\n","        # if (epoch != 0) and (epoch % 200) == 0:\n","        #     sess.run(tf.initialize_variables(adam_vars))\n","        #     print(\"Re-initialize Adam\")\n","\n","\n","if __name__ == '__main__':\n","    import argparse\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument('--mode', type=str, default=\"train\",\n","                       help='train, train_encoder, translation')\n","\n","    args = parser.parse_args()\n","\n","    if args.mode == \"train\":\n","        main_train()\n"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-95a1c17f5a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"jWp-FCJzcmF0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"outputId":"8d33ddcc-cf48-4c94-af57-92399400bb63","executionInfo":{"status":"ok","timestamp":1541945433049,"user_tz":-330,"elapsed":6058,"user":{"displayName":"Manoj K Sharma","photoUrl":"https://lh5.googleusercontent.com/-g6L3ibHeLnU/AAAAAAAAAAI/AAAAAAAABGo/TXgjrDnzaEM/s64/photo.jpg","userId":"14163422646141132762"}}},"cell_type":"code","source":["!pip3 install tensorlayer==1.7.0"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting tensorlayer==1.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/54/4fe45b1fa228df243fa07ec1d18533b6cdbb3a9826412c6ec82b30b6d765/tensorlayer-1.7.0.zip (257kB)\n","\r\u001b[K    3% |█▎                              | 10kB 19.5MB/s eta 0:00:01\r\u001b[K    7% |██▌                             | 20kB 2.8MB/s eta 0:00:01\r\u001b[K    11% |███▉                            | 30kB 3.2MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 40kB 3.0MB/s eta 0:00:01\r\u001b[K    19% |██████▍                         | 51kB 3.4MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 61kB 4.0MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 71kB 3.9MB/s eta 0:00:01\r\u001b[K    31% |██████████▏                     | 81kB 3.8MB/s eta 0:00:01\r\u001b[K    35% |███████████▌                    | 92kB 4.3MB/s eta 0:00:01\r\u001b[K    39% |████████████▊                   | 102kB 4.4MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 112kB 4.5MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 122kB 5.6MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 133kB 5.5MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▉              | 143kB 6.8MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 153kB 6.4MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▍           | 163kB 5.9MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 174kB 6.9MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 184kB 7.4MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▏       | 194kB 7.4MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 204kB 7.3MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▊     | 215kB 6.6MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 225kB 7.6MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▎  | 235kB 7.4MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 245kB 6.7MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 256kB 7.7MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 266kB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorlayer==1.7.0) (1.14.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensorlayer==1.7.0) (1.1.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from tensorlayer==1.7.0) (0.13.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tensorlayer==1.7.0) (2.1.2)\n","Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.6/dist-packages (from scikit-image->tensorlayer==1.7.0) (1.11.0)\n","Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->tensorlayer==1.7.0) (4.0.0)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->tensorlayer==1.7.0) (1.0.1)\n","Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image->tensorlayer==1.7.0) (2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorlayer==1.7.0) (2.5.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorlayer==1.7.0) (2018.7)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorlayer==1.7.0) (2.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorlayer==1.7.0) (0.10.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image->tensorlayer==1.7.0) (0.46)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image->tensorlayer==1.7.0) (4.3.0)\n","Building wheels for collected packages: tensorlayer\n","  Running setup.py bdist_wheel for tensorlayer ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/dc/28/2d/b74603a4020a4d96d69ca66cc04d94304fcf3388f28356f93c\n","Successfully built tensorlayer\n","Installing collected packages: tensorlayer\n","  Found existing installation: tensorlayer 1.4.1\n","    Uninstalling tensorlayer-1.4.1:\n","      Successfully uninstalled tensorlayer-1.4.1\n","Successfully installed tensorlayer-1.7.0\n"],"name":"stdout"}]}]}